---
title: "Training the model"
author: "Nathanael Aff"
output: 
  html_document:
    code_folding: hide
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->

```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk(here::here("code", "chunk-options.R"))
```

```{r knitr-opts-chunk, include=FALSE}
```
<!-- Insert last udpate and git version-->
```{r last-updated, echo=FALSE, results='asis'}
```

```{r code-version, echo=FALSE, results='asis'}
```
 
# Selecting a topic model 

As with kmeans clustering the LDA model requires the number of topics $k$ to be selected by the user. In this section we test out several methods that could be used to automate the scoring and selection

## Cross validation on perplexity 

The LDA model learns to posterior distributions which are the optimization routine's best guess at the distributions that generated the data. One method to test how good those distributions fit our data is to compare the learned distribution on a training set to the distribution of a holdout set. Perplexity is one measure of the difference between estimated topic distributions on documents. 

We cast the set_word table to a document term matrix using the `tidytext` function. This returns a` documentTermMatrix` object from the `tm` package. The LDA function we use is from the `topicmodels` package. It has a variational expectation maximation method and a Gibbs sampling method and I used the former.

```{r load-data, eval = TRUE, echo = FALSE}
devtools::load_all()
knitr::read_chunk(here::here("code", "perplexity-cv.R"))
```

```{r setup, eval = TRUE, echo = FALSE}
```

```{r k-fold-cv, eval=FALSE, echo=FALSE, warning=FALSE}
```

```{r load-results, eval=TRUE, echo=TRUE}
```

### K-topic grid

Since I tested the running time on different set numbers I have seen that for this small number of sets there aren't many topics. I have included a few more topic values $k$ on the lower end but including some higher values to better see the trend.


```{r cv-result-plot, eval=TRUE, echo=TRUE, fig.height = 4, fig.width = 7}
```

```{r lda-models, eval=TRUE, echo=TRUE}
```

## `ldatuning` topic scores 

The `ldatuning` package has several other metrics of the quality of the topic models. I have modified the main function from the package to only return the scores. (The original package computes the models first and then the scores).  

```{r read-ldatuning}
knitr::read_chunk(here::here("code", "ldatuning-scores.R"))
```

```{r ldatuning-scores, eval=TRUE, fig.height = 4, fig.width = 7}
```

## Topic coherence

There are several version of topic coherence which measure the pairwise strength of the relationship of the top terms in a topic model. Given some score where a larger value indicates a stronger relation ship between two words $w_i, w_j$, a generic coherence score is the sum over the top terms in a topic model:  
 $$ \sum_{w_i, w_j \in W_t} \text{Score}(w_i, w_j), $$
 with top terms $W_t$ for each topic $t$.
 
The coherence score used in the `SpeedReader` coherence function just uses the internal coherence of the top terms. I compared the scores for the top 3, 5 and 10 terms.

```{r read-coherence, eval = TRUE, echo = FALSE}
knitr::read_chunk(here::here("code", "coherence-scores.R"))
```

```{r coherence-score, eval = TRUE, echo = TRUE, fig.height = 4, fig.width = 7}
```


## Cluster scoring 
We can also treat the LDA models as clustering the LEGO sets. We can assign the LEGO set to the color topic which has the highest value for that document; This is the topic that is most responsible for generating the document.

The previous plot should indicate whether documents are getting strongly associated with a topic or if topics are to evenly distributed over all documents. 

### Clustering sets with kmeans
In this next section, I cluster documents using both kmeans and LDA topics. Kmeans is intended as a simple baseline clustering method and sets are clustered based on their term vectors weighted by TF-IDF scores. 

### Cluster analysis

The clusters scores include Rand, adjusted Rand, Folkes-Mallow and Jaccard scores. All try to score a clustering on how well the discovered labels match the assigned labels -- here the `root_id` of the set.  The Rand index assigns a score based on the number pairwise agreement of the cluster labels with the original labels. The other measures are somewhat similar in approach.

```{r read-compare-clusters, eval = TRUE, echo = FALSE}
devtools::load_all()
knitr::read_chunk(here::here("code", "compare-cluster-scores.R"))

```

```{r plot-external-scores2, eval = TRUE, echo = FALSE, fig.height = 4,fig.width = 7}

```

## Topic Distribution

Another way to evaluate the quality of the topic models is to see how well documents are sorted into topics. This example follows a [this section](http://tidytextmining.com/nasa.html#calculating-tf-idf-for-the-description-fields) from the tidy text mining book. 

The topic model's gamma matrix has the distribution of topics over models: 
\[
 \text{gamma}  = p(t|d)
\] 
for topic $t$ and document $d$.

The plot below visualizes this as how the _topics_ are distributed over the probability bins for each topic. If too many topics have sets or documents in the low probability bins then you may have too high a number of topics since few documents are being strongly associated with any topic. 


```{r read-distributions, eval = TRUE, echo = FALSE}
knitr::read_chunk(here::here("code", "gamma-distribution.R"))
```


Following the tidytext book, look at the distribution over topics.

```{r load-distributions, eval = TRUE, echo = FALSE}
```


```{r  plot-40-topics, eval = TRUE, echo = FALSE, fig.height = 7, fig.width = 9}
```